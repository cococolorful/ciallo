强化学习
==========

强化学习旨在解决 **决策** 问题。例如，在十字路口等待交通信号灯时，面对红灯我们应该等待还是继续前进？面对绿灯时呢？我们应该采取同样的动作吗？

对于一些简单的决策问题，我们可以依靠固定的规则进行解决。例如，面对红灯时就停止，面对绿灯时就前进。然而，对于复杂的决策问题，我们往往无法找到固定的规则，此时强化学习是一条可以尝试的方案。

强化学习的基本思想是，通过不断尝试，找到最优的决策策略。例如，在十字路口等待交通信号灯时，我们可以尝试等待和前进，然后根据结果选择最优的策略。

基本概念
--------
具体而言，强化学习把决策问题建模成 **智能体（Agent）** 和 **环境（Environment）** 之间的 **交互过程**。如下图所示，在整个过程中，智能体根据当前状态选择一个动作，环境根据智能体的动作返回下一个状态和奖励。通过最大化可以得到的奖励，优化智能体的决策能力，即根据当前状态选择一个 **最佳** 的动作。

基本要素
--------
强化学习主要包括以下基本要素：

状态（State）
^^^^^^^^^^^
状态 :math:`S` 指的是当前环境的状态
- （State）：当前环境的状态，记为 。

动作空间（Action Space）
^^^^^^^^^^^^^^^^^^^^^^^^
：在当前状态下可以采取的行动空间，记为 :math:`A`。

策略（Policy）
^^^^^^^^^^^^^
：在给定状态下，选择某个行动的规则，记为 :math:`\pi`。

奖励函数（Reward Function）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
- ：采取某个行动后，环境给予的反馈，记为 :math:`R`。

他们之间存在着一定的联系：

- 奖励与当前的状态和采取的行动有关，即 :math:`r = R(s, a)`
- 策略决定了在当前状态下采取哪个行动，即 :math:`a = \pi(s)`
- 状态转移函数决定了在当前状态下采取某个行动后，下一个状态是什么，即 :math:`s' = T(s, a)`

轨迹（Trajectory）
^^^^^^^^^^^^^^
由于RL是一个交互过程，于是又衍生出一个概念叫做 **轨迹（Trajectory）**，它表示的是智能体在环境中的行动轨迹，记为 :math:`\tau = (s_0, a_0, s_1, a_1, s_2, a_2, \cdots)`。

其中，环境的初始状态 :math:`s_0` 是一个随机变量，而后续的状态 :math:`s_1, s_2, \cdots` 是由状态转移函数 :math:`T` 和策略 :math:`\pi` 决定的:

.. math::
    s_{t+1} = T(s_t, a_t)

    a_t = \pi(s_t)


回报（Return）
^^^^^^^^^^
强化学习是一个试错的过程，我们需要在每一个时刻，对智能体的行为进行评价，即奖励函数 :math:`R`。其中，对于t时刻的奖励，记为 :math:`r_t`，公式表示为

.. math::
    r_t = R(s_t, a_t)

.. math::

然而，仅仅考虑当前时刻的奖励，并不能够很好地评价智能体的行为，因为智能体的行为往往会对后续的状态产生影响。因此，我们引入了 **回报（Return）** 的概念，它表示的是智能体在 :math:`t` 时刻及以后时刻得到的奖励之和，即：

.. math::
    G(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t

其中， :math:`\gamma` 是一个介于0和1之间的折扣因子，它表示了智能体对未来的奖励的重视程度。当 :math:`\gamma = 0` 时，智能体只考虑当前时刻的奖励，而忽略未来的奖励；当 :math:`\gamma = 1` 时，智能体对未来的奖励给予与当前时刻奖励相同的重要性。

基本问题
--------
强化学习需要解决的问题，就是使得智能体在给定策略 :math:`\pi` 下，能够最大化期望回报 :math:`\mathbb{E}[G|\pi]`。

在已知策略 :math:`\pi` 和环境状态转移方程的情况下，我们可以先得到轨迹的分布。

.. math::
    P(\tau | \pi) = P(s_0) \prod_{t=0}^{\infty} P(s_{t+1} | s_t, a_t) P(a_t | s_t, \pi)

然后，在此策略 :math:`\pi` 下的期望回报可以表示为:

.. math::
    J(\pi) = \mathbb{E}[G|\pi] = \int G(\tau) P(\tau | \pi) d\tau

因此，强化学习的基本问题，就是找到最优策略 :math:`\pi^*`，使得期望回报 最大，即:

.. math::
    \pi^* = \arg \max_{\pi} J(\pi)

值函数（Value Function）
^^^^^^^^^^
值函数（Value Function）是强化学习中的一个重要概念，它表示的是智能体在给定策略 :math:`\pi` 下，从状态 :math:`s` 出发，期望得到的回报。值函数可以分为两种：

1. 状态值函数（State Value Function）：表示的是智能体在给定策略 :math:`\pi` 下，从状态 :math:`s` 出发，期望得到的回报。

.. math::
    v_{\pi}(s) = \mathbb{E}[G | s_0 = s, \pi]
2. 动作值函数（Action Value Function）：表示的是智能体在给定策略 :math:`\pi` 下，从状态 :math:`s` 出发，执行动作 :math:`a`，期望得到的回报。

.. math::
    q_{\pi}(s, a) = \mathbb{E}[G | s_0 = s, a_0 = a, \pi]